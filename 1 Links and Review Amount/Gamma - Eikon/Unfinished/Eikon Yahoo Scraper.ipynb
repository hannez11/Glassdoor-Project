{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from torpy.http.requests import TorRequests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv(r\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\2 Eikon Sample\\0930_MnA_Sample_Eikon_AcqAndTargets_forYahoo.csv\", sep=\";\", decimal=\",\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yahoo scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/68954543\n",
    "import functools\n",
    "def update_class(\n",
    "    main_class=None, exclude=(\"__module__\", \"__name__\", \"__dict__\", \"__weakref__\")\n",
    "):\n",
    "    \"\"\"Class decorator. Adds all methods and members from the wrapped class to main_class\n",
    "\n",
    "    Args:\n",
    "    - main_class: class to which to append members. Defaults to the class with the same name as the wrapped class\n",
    "    - exclude: black-list of members which should not be copied\n",
    "    \"\"\"\n",
    "\n",
    "    def decorates(main_class, exclude, appended_class):\n",
    "        if main_class is None:\n",
    "            main_class = globals()[appended_class.__name__]\n",
    "        for k, v in appended_class.__dict__.items():\n",
    "            if k not in exclude:\n",
    "                setattr(main_class, k, v)\n",
    "        return main_class\n",
    "\n",
    "    return functools.partial(decorates, main_class, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooScraperClass:\n",
    "    def yahoo_scraper(self, df, sleeptime_max=1, tor_active=\"no\"):\n",
    "        try: # if operation fails, then return the current df\n",
    "            firmname = df[\"full_name_cleaned\"]\n",
    "            print(f\"Firm #{df.name + 1}: {firmname}\")\n",
    "            # query = f'site:glassdoor.com {firmname}'.replace(' ', '+') # non-strict\n",
    "            query = f'site:glassdoor.com \"{firmname}\"'.replace(' ', '+') #putting {firmname} around \"\" might help. however, the query might also be too strict not returning any results\n",
    "            URL = f\"https://search.yahoo.com/search?p={query}&vc=en&pz=20\"\n",
    "\n",
    "            # shuffle through user agent list for bot prevention. use IP shuffling to prevent further bot detection\n",
    "            time.sleep(round(random.uniform(0.25,sleeptime_max), 3))\n",
    "            USER_AGENTS = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:103.0) Gecko/20100101 Firefox/103.0\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 Edg/103.0.1264.71\"\n",
    "                        ]\n",
    "            headers = {\"user-agent\":random.choice(USER_AGENTS)}\n",
    "\n",
    "            ## use tor\n",
    "            if tor_active==\"yes\":\n",
    "                try:\n",
    "                    url = self.sess.get(URL, headers=headers)\n",
    "                except:\n",
    "                    print(\"Error getting the URL with tor requests\")\n",
    "            ## use regular IP\n",
    "            else:\n",
    "                url = requests.get(URL, headers=headers)\n",
    "\n",
    "            soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "            time.sleep(0.2) #to fully load the page\n",
    "\n",
    "            ## scraper ##\n",
    "            #############\n",
    "            link_found = 0\n",
    "            ## check if amount of ratings is displayed in at least of the search results. if so, grab this number and the parent div containing the link\n",
    "            if soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) and len(soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}).text)<15: #check length of the classes text, since class is not exclusively used for amount of ratings\n",
    "                ## grab the review number\n",
    "                review_amount = soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) #only get the first item, since it appears to be most accurate. could be of the last search result\n",
    "                try: #sometimes \"Currency: EUR\" is displayed instead of the review amount\n",
    "                    review_amount_final = review_amount.text.split(\"(\")[1][:-1] #\"4/5 (3)\"\n",
    "                    if \"K\" in review_amount_final: # \"5.7K\" to 5700\n",
    "                        review_amount_final = review_amount_final.replace(\".\", \"\")\n",
    "                        review_amount_final = review_amount_final.replace(\"K\", \"00\")\n",
    "                    # print(f\"review amount: {review_amount_final}\")\n",
    "                    df[\"review_amount\"] = review_amount_final\n",
    "                    ## grab the href from the parent of the same search result\n",
    "                except:\n",
    "                    df[\"review_amount\"] = \"\" \n",
    "                review_parents = review_amount.find_parents(\"div\")\n",
    "                for link in review_parents:\n",
    "                    if link.find(\"a\") and \"glassdoor\" in link.find(\"a\")[\"href\"]:\n",
    "                        alink = link.find(\"a\")\n",
    "                        link_found = 1\n",
    "                        break\n",
    "            ## if amount of ratings is not on page, then just grab the href\n",
    "            elif link_found != 1:\n",
    "                # print(\"review amount: none found\")\n",
    "                results = soup.findAll(\"h3\")\n",
    "                for link in results:\n",
    "                    if link.find(\"a\") and \"glassdoor\" in link.find(\"a\")[\"href\"]:\n",
    "                        alink = link.find(\"a\")\n",
    "                        link_found = 1\n",
    "                        break\n",
    "                ## if neither link nor review amount was found\n",
    "                if link_found != 1:\n",
    "                    link_found = \"no link found\" #\"no link found\"\n",
    "                    link_final = \"\"\n",
    "                    print(\"no link found on first page\")\n",
    "            \n",
    "            ## decode href link if link was found ##\n",
    "            ########################################\n",
    "            if link_found == 1:\n",
    "                link_with_yahoo = unquote(alink[\"href\"]) #unquote decodes URLs\n",
    "                try: # cover weird cases in which an image link is retrieved\n",
    "                    link_final = link_with_yahoo.split(\"RU=\")[1].split(\"/RK=2\")[0] #https://r.search.yahoo.com/_ylt=A0geK9dD9t9i2BIAtodXNyoA;_ylu=Y29sbwNiZjEEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1658873541/RO=10/RU=https://www.glassdoor.com/Overview/Working-at-California-Micro-Devices-EI_IE1221.11,35.htm/RK=2/RS=Ss7CVR85Jhnt899NkWVSw9eSVUw-\n",
    "                    if \"/Working\" in link_final:\n",
    "                        # from https://www.glassdoor.com/Overview/Working-at-Tesla-EI_IE43129.11,16.htm\n",
    "                        # to https://www.glassdoor.com/Reviews/Tesla-Reviews-E43129.htm\n",
    "                        link_final = link_final.replace(\"/Overview/Working-at-\", \"/Reviews/\") #shouldnt be an issue to replace the /Overview/ part with /Reviews/ even though the original Reviews link looks different\n",
    "                        link_final = link_final.replace(\"EI_IE\", \"Reviews-E\") \n",
    "                        link_final = link_final.split(\".htm\")[0][:-6] + \".htm\"\n",
    "                        link_found = \"transformed overview link\"\n",
    "                    elif \"/Jobs/\" in link_final:\n",
    "                        link_final = link_final.replace(\"/Jobs/\", \"/Reviews/\")\n",
    "                        link_final = link_final.replace(\"-Jobs-\", \"-Reviews-\")\n",
    "                        link_found = \"transformed jobs link\"\n",
    "                    elif \"/Salary/\" in link_final:\n",
    "                        link_final = link_final.replace(\"/Salary/\", \"/Reviews/\")\n",
    "                        link_final = link_final.replace(\"-Salaries-\", \"-Reviews-\")\n",
    "                        link_found = \"transformed salary link\"\n",
    "                    elif \"/Reviews/\" in link_final:\n",
    "                        link_found = \"original review link\"\n",
    "                    else:\n",
    "                    #elif \"/job-listing/\" or \"/Job/\" in link_final:\n",
    "                        link_found = \"wrong link\"\n",
    "                except:\n",
    "                    link_final = link_with_yahoo\n",
    "                    \n",
    "            df[\"link_yahoo\"] = link_final\n",
    "            df[\"link_found\"] = link_found\n",
    "            print(link_final, \"\\n\")\n",
    "            return df\n",
    "            \n",
    "        except:\n",
    "            print(\"Some error\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yahoo scraper with tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class YahooScraperClass:\n",
    "    def tor_yahoo_scraper(self, df_dynamic, startrow=0, endrow=5, tor=\"no\", sleeptime_max=0.5):\n",
    "        ## set parameters\n",
    "        # tor: change to yes or no\n",
    "        # sleeptime_max: use >2 with original ip (tor = \"no\"). around 1000 requests possible until temp. ip block\n",
    "\n",
    "        ## create a copy of the input frame; keep old index as extra column\n",
    "        df_dynamic = input[startrow:endrow].reset_index(drop=False)\n",
    "\n",
    "        df_dynamic[\"link_yahoo\"] = np.nan\n",
    "        df_dynamic[\"review_amount\"] = np.nan\n",
    "        df_dynamic[\"link_found\"] = np.nan\n",
    "\n",
    "        ## with Tor-Gateway\n",
    "        if tor == \"yes\":\n",
    "            with TorRequests() as tor_requests:\n",
    "                print(\"Connecting to tor..\")\n",
    "                with tor_requests.get_session() as self.sess: #sess pulls a request of the page with the tor connection. self is crucial in front of sess. otherwise wont work in the yahoo_scraper method\n",
    "                    current_ip = self.sess.get(\"http://httpbin.org/ip\").json()\n",
    "                    print(f\"Current IP:{current_ip}; sleeptime: {sleeptime_max}\\n\")\n",
    "                    df_dynamic = df_dynamic.apply(self.yahoo_scraper, args=(sleeptime_max, \"yes\"), axis=1)\n",
    "        ## without tor\n",
    "        else:\n",
    "            df_dynamic = df_dynamic.apply(self.yahoo_scraper, args=(sleeptime_max, \"no\"), axis=1)\n",
    "\n",
    "        print(f\"Scraping finished for {startrow} to {endrow}\")\n",
    "        \n",
    "        ## save file with dynamic file name\n",
    "        dir = Path(r\"C:\\Users\\Hannes\\Desktop\\dfs\")\n",
    "        path_with_time = Path.joinpath(dir, f'{time.strftime(\"%m%d\")}_MnA_Sample_Eikon_{startrow}_{endrow}.csv')\n",
    "        df_dynamic.to_csv(path_with_time, sep=\";\", decimal=\",\", float_format='%.3f', index=False)\n",
    "        print(f\"{path_with_time} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df31 = pd.DataFrame()\n",
    "YahooScraperClass().tor_yahoo_scraper(df31, startrow=0, endrow=4, tor=\"no\", sleeptime_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create args for tor_yahoo_scraper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('df_mp0', 0, 499, 'no', 0.5), ('df_mp1', 500, 999, 'no', 0.5), ('df_mp2', 1000, 1499, 'no', 0.5), ('df_mp3', 1500, 1999, 'no', 0.5), ('df_mp4', 2000, 2499, 'no', 0.5), ('df_mp5', 2500, 2999, 'no', 0.5), ('df_mp6', 3000, 3499, 'no', 0.5), ('df_mp7', 3500, 3999, 'no', 0.5), ('df_mp8', 4000, 4499, 'no', 0.5), ('df_mp9', 4500, 4999, 'no', 0.5), ('df_mp10', 5000, 5499, 'no', 0.5), ('df_mp11', 5500, 5999, 'no', 0.5), ('df_mp12', 6000, 6499, 'no', 0.5), ('df_mp13', 6500, 6999, 'no', 0.5), ('df_mp14', 7000, 7499, 'no', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "@update_class()\n",
    "class YahooScraperClass:\n",
    "    def create_pool_args(self,\n",
    "        first_row=0,\n",
    "        lastrow=input.shape[0], \n",
    "        rows_per_pool=500):\n",
    "\n",
    "        startrows = [i for i in range (first_row,lastrow,rows_per_pool)]\n",
    "        endrows = [i for i in range (rows_per_pool-1,lastrow,rows_per_pool)]\n",
    "\n",
    "        dfs_multi_namelist = [f\"df_mp{i}\" for i in range(len(startrows))]\n",
    "\n",
    "        dfs_multi = {name: pd.DataFrame() for name in dfs_multi_namelist}\n",
    "\n",
    "        self.pool_args = list(zip(dfs_multi_namelist, startrows, endrows, repeat(\"no\"), repeat(0.5)))\n",
    "        print(self.pool_args)\n",
    "\n",
    "YahooScraperClass().create_pool_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caae7e249fc351f228f30c0ea597686a1b1aad5e394e945128343f5494793c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
