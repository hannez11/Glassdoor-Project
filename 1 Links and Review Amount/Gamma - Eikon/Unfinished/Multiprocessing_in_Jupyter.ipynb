{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from torpy.http.requests import TorRequests\n",
    "\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv(r\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\2 Eikon Sample\\0930_MnA_Sample_Eikon_AcqAndTargets_forYahoo.csv\", sep=\";\", decimal=\",\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yahoo scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/68954543\n",
    "import functools\n",
    "def update_class(\n",
    "    main_class=None, exclude=(\"__module__\", \"__name__\", \"__dict__\", \"__weakref__\")\n",
    "):\n",
    "    \"\"\"Class decorator. Adds all methods and members from the wrapped class to main_class\n",
    "\n",
    "    Args:\n",
    "    - main_class: class to which to append members. Defaults to the class with the same name as the wrapped class\n",
    "    - exclude: black-list of members which should not be copied\n",
    "    \"\"\"\n",
    "\n",
    "    def decorates(main_class, exclude, appended_class):\n",
    "        if main_class is None:\n",
    "            main_class = globals()[appended_class.__name__]\n",
    "        for k, v in appended_class.__dict__.items():\n",
    "            if k not in exclude:\n",
    "                setattr(main_class, k, v)\n",
    "        return main_class\n",
    "\n",
    "    return functools.partial(decorates, main_class, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooScraperClass:\n",
    "    def yahoo_scraper(self, df, sleeptime_max=1, tor_active=\"no\"):\n",
    "        try: # if operation fails, then return the current df\n",
    "            firmname = df[\"full_name_cleaned\"]\n",
    "            print(f\"Firm #{df.name + 1}: {firmname}\")\n",
    "            # query = f'site:glassdoor.com {firmname}'.replace(' ', '+') # non-strict\n",
    "            query = f'site:glassdoor.com \"{firmname}\"'.replace(' ', '+') #putting {firmname} around \"\" might help. however, the query might also be too strict not returning any results\n",
    "            URL = f\"https://search.yahoo.com/search?p={query}&vc=en&pz=20\"\n",
    "\n",
    "            # shuffle through user agent list for bot prevention. use IP shuffling to prevent further bot detection\n",
    "            time.sleep(round(random.uniform(0.25,sleeptime_max), 3))\n",
    "            USER_AGENTS = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:103.0) Gecko/20100101 Firefox/103.0\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36\",\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 Edg/103.0.1264.71\"\n",
    "                        ]\n",
    "            headers = {\"user-agent\":random.choice(USER_AGENTS)}\n",
    "\n",
    "            ## use tor\n",
    "            if tor_active==\"yes\":\n",
    "                try:\n",
    "                    url = self.sess.get(URL, headers=headers)\n",
    "                except:\n",
    "                    print(\"Error getting the URL with tor requests\")\n",
    "            ## use regular IP\n",
    "            else:\n",
    "                url = requests.get(URL, headers=headers)\n",
    "\n",
    "            soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "            time.sleep(0.2) #to fully load the page\n",
    "\n",
    "            ## scraper ##\n",
    "            #############\n",
    "            link_found = 0\n",
    "            ## check if amount of ratings is displayed in at least of the search results. if so, grab this number and the parent div containing the link\n",
    "            if soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) and len(soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}).text)<15: #check length of the classes text, since class is not exclusively used for amount of ratings\n",
    "                ## grab the review number\n",
    "                review_amount = soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) #only get the first item, since it appears to be most accurate. could be of the last search result\n",
    "                try: #sometimes \"Currency: EUR\" is displayed instead of the review amount\n",
    "                    review_amount_final = review_amount.text.split(\"(\")[1][:-1] #\"4/5 (3)\"\n",
    "                    if \"K\" in review_amount_final: # \"5.7K\" to 5700\n",
    "                        review_amount_final = review_amount_final.replace(\".\", \"\")\n",
    "                        review_amount_final = review_amount_final.replace(\"K\", \"00\")\n",
    "                    # print(f\"review amount: {review_amount_final}\")\n",
    "                    df[\"review_amount\"] = review_amount_final\n",
    "                    ## grab the href from the parent of the same search result\n",
    "                except:\n",
    "                    df[\"review_amount\"] = \"\" \n",
    "                review_parents = review_amount.find_parents(\"div\")\n",
    "                for link in review_parents:\n",
    "                    if link.find(\"a\") and \"glassdoor\" in link.find(\"a\")[\"href\"]:\n",
    "                        alink = link.find(\"a\")\n",
    "                        link_found = 1\n",
    "                        break\n",
    "            ## if amount of ratings is not on page, then just grab the href\n",
    "            elif link_found != 1:\n",
    "                # print(\"review amount: none found\")\n",
    "                results = soup.findAll(\"h3\")\n",
    "                for link in results:\n",
    "                    if link.find(\"a\") and \"glassdoor\" in link.find(\"a\")[\"href\"]:\n",
    "                        alink = link.find(\"a\")\n",
    "                        link_found = 1\n",
    "                        break\n",
    "                ## if neither link nor review amount was found\n",
    "                if link_found != 1:\n",
    "                    link_found = \"no link found\" #\"no link found\"\n",
    "                    link_final = \"\"\n",
    "                    print(\"no link found on first page\")\n",
    "            \n",
    "            ## decode href link if link was found ##\n",
    "            ########################################\n",
    "            if link_found == 1:\n",
    "                link_with_yahoo = unquote(alink[\"href\"]) #unquote decodes URLs\n",
    "                try: # cover weird cases in which an image link is retrieved\n",
    "                    link_final = link_with_yahoo.split(\"RU=\")[1].split(\"/RK=2\")[0] #https://r.search.yahoo.com/_ylt=A0geK9dD9t9i2BIAtodXNyoA;_ylu=Y29sbwNiZjEEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1658873541/RO=10/RU=https://www.glassdoor.com/Overview/Working-at-California-Micro-Devices-EI_IE1221.11,35.htm/RK=2/RS=Ss7CVR85Jhnt899NkWVSw9eSVUw-\n",
    "                    if \"/Working\" in link_final:\n",
    "                        # from https://www.glassdoor.com/Overview/Working-at-Tesla-EI_IE43129.11,16.htm\n",
    "                        # to https://www.glassdoor.com/Reviews/Tesla-Reviews-E43129.htm\n",
    "                        link_final = link_final.replace(\"/Overview/Working-at-\", \"/Reviews/\") #shouldnt be an issue to replace the /Overview/ part with /Reviews/ even though the original Reviews link looks different\n",
    "                        link_final = link_final.replace(\"EI_IE\", \"Reviews-E\") \n",
    "                        link_final = link_final.split(\".htm\")[0][:-6] + \".htm\"\n",
    "                        link_found = \"transformed overview link\"\n",
    "                    elif \"/Jobs/\" in link_final:\n",
    "                        link_final = link_final.replace(\"/Jobs/\", \"/Reviews/\")\n",
    "                        link_final = link_final.replace(\"-Jobs-\", \"-Reviews-\")\n",
    "                        link_found = \"transformed jobs link\"\n",
    "                    elif \"/Salary/\" in link_final:\n",
    "                        link_final = link_final.replace(\"/Salary/\", \"/Reviews/\")\n",
    "                        link_final = link_final.replace(\"-Salaries-\", \"-Reviews-\")\n",
    "                        link_found = \"transformed salary link\"\n",
    "                    elif \"/Reviews/\" in link_final:\n",
    "                        link_found = \"original review link\"\n",
    "                    else:\n",
    "                    #elif \"/job-listing/\" or \"/Job/\" in link_final:\n",
    "                        link_found = \"wrong link\"\n",
    "                except:\n",
    "                    link_final = link_with_yahoo\n",
    "                    \n",
    "            df[\"link_yahoo\"] = link_final\n",
    "            df[\"link_found\"] = link_found\n",
    "            print(link_final, \"\\n\")\n",
    "            return df\n",
    "            \n",
    "        except:\n",
    "            print(\"Some error\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yahoo scraper with tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class YahooScraperClass:\n",
    "    def tor_yahoo_scraper(self, df_dynamic, startrow=0, endrow=5, tor=\"no\", sleeptime_max=0.5):\n",
    "        ## set parameters\n",
    "        # tor: change to yes or no\n",
    "        # sleeptime_max: use >2 with original ip (tor = \"no\"). around 1000 requests possible until temp. ip block\n",
    "\n",
    "        ## create a copy of the input frame; keep old index as extra column\n",
    "        df_dynamic = input[startrow:endrow].reset_index(drop=False)\n",
    "\n",
    "        df_dynamic[\"link_yahoo\"] = np.nan\n",
    "        df_dynamic[\"review_amount\"] = np.nan\n",
    "        df_dynamic[\"link_found\"] = np.nan\n",
    "\n",
    "        ## with Tor-Gateway\n",
    "        if tor == \"yes\":\n",
    "            with TorRequests() as tor_requests:\n",
    "                print(\"Connecting to tor..\")\n",
    "                with tor_requests.get_session() as self.sess: #sess pulls a request of the page with the tor connection. self is crucial in front of sess. otherwise wont work in the yahoo_scraper method\n",
    "                    # current_ip = sess.get(\"http://httpbin.org/ip\").json()\n",
    "                    # print(f\"Current IP:{current_ip}; sleeptime: {sleeptime_max}\\n\")\n",
    "                    df_dynamic = df_dynamic.apply(self.yahoo_scraper, args=(sleeptime_max, \"yes\"), axis=1) #sess method can be executed from this sibling function\n",
    "        ## without tor\n",
    "        else:\n",
    "            df_dynamic = df_dynamic.apply(self.yahoo_scraper, args=(sleeptime_max, \"no\"), axis=1)\n",
    "\n",
    "        print(f\"Scraping finished for {startrow} to {endrow}\")\n",
    "        \n",
    "        ## save file with dynamic file name\n",
    "        # dir = Path(r\"C:\\Users\\Hannes\\Desktop\\dfs\")\n",
    "        # path_with_time = Path.joinpath(dir, f'{time.strftime(\"%m%d\")}_MnA_Sample_Eikon_{startrow}_{endrow}.csv')\n",
    "        # df_dynamic.to_csv(path_with_time, sep=\";\", decimal=\",\", float_format='%.3f', index=False) #english decimal\n",
    "        # print(f\"{path_with_time} saved\")\n",
    "\n",
    "        return df_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df31 = pd.DataFrame()\n",
    "YahooScraperClass().tor_yahoo_scraper(df31, startrow=0, endrow=4, tor=\"yes\", sleeptime_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "# @update_class()\n",
    "class YahooScraperClass:\n",
    "    def create_pool_args(self,\n",
    "        first_row=0,\n",
    "        lastrow=input.shape[0], \n",
    "        rows_per_pool=500):\n",
    "\n",
    "        startrows = [i for i in range (first_row,lastrow,rows_per_pool)]\n",
    "        endrows = [i for i in range (rows_per_pool-1,lastrow,rows_per_pool)]\n",
    "\n",
    "        dfs_multi_namelist = [f\"df_mp{i}\" for i in range(len(startrows))]\n",
    "\n",
    "        dfs_multi = {name: pd.DataFrame() for name in dfs_multi_namelist}\n",
    "\n",
    "        self.pool_args = list(zip(dfs_multi_namelist, startrows, endrows, repeat(\"no\"), repeat(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import workersclass\n",
    "\n",
    "@update_class()\n",
    "class YahooScraperClass:\n",
    "    def YahooScraperMultiProcess(self):\n",
    "        if __name__==\"__main__\":\n",
    "            self.create_pool_args(first_row=0,lastrow=10, rows_per_pool=5)\n",
    "            # Pool().starmap(workers.tor_yahoo_scraper, [(df31, 0, 3, \"no\", 0.5), (df32, 4, 6, \"no\", 0.5)])\n",
    "            print(f\"pools_args: {self.pool_args}\")\n",
    "            # Pool().starmap(workersclass.YahooScraperClass().tor_yahoo_scraper, self.pool_args)\n",
    "            p1 = mp.Process(target=workersclass.YahooScraperClass().tor_yahoo_scraper, args=self.pool_args[0])\n",
    "            p2 = mp.Process(target=workersclass.YahooScraperClass().tor_yahoo_scraper, args=self.pool_args[1])\n",
    "\n",
    "            p1.start()\n",
    "            p2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import workersclass\n",
    "\n",
    "@update_class()\n",
    "class YahooScraperClass:\n",
    "    def YahooScraperMulti(self):\n",
    "        self.df31 = pd.DataFrame()\n",
    "        self.df32 = pd.DataFrame()\n",
    "        # self.create_pool_args(first_row=0,lastrow=10, rows_per_pool=5)\n",
    "        # print(f\"pools_args: {self.pool_args}\")\n",
    "        Pool().starmap(workersclass.YahooScraperClass2().tor_yahoo_scraper, [(self.df31, 0, 3, \"no\", 0.5), (self.df32, 4, 6, \"no\", 0.5)])\n",
    "        # Pool(2).starmap(workersclass.YahooScraperClass2().tor_yahoo_scraper, self.pool_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    YahooScraperClass().YahooScraperMulti()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessing.process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import workers\n",
    "\n",
    "df31 = pd.DataFrame()\n",
    "df32 = pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p1 = mp.Process(target=workers.tor_yahoo_scraper, kwargs={\"df_dynamic\": df31, \"startrow\": 0, \"endrow\": 5, \"tor\": \"no\", \"sleeptime_max\":1})\n",
    "    p2 = mp.Process(target=workers.tor_yahoo_scraper, kwargs={\"df_dynamic\": df32, \"startrow\": 6, \"endrow\": 10, \"tor\": \"no\", \"sleeptime_max\":1})\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split class into multiple cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/68954543\n",
    "import functools\n",
    "def update_class(\n",
    "    main_class=None, exclude=(\"__module__\", \"__name__\", \"__dict__\", \"__weakref__\")\n",
    "):\n",
    "    \"\"\"Class decorator. Adds all methods and members from the wrapped class to main_class\n",
    "\n",
    "    Args:\n",
    "    - main_class: class to which to append members. Defaults to the class with the same name as the wrapped class\n",
    "    - exclude: black-list of members which should not be copied\n",
    "    \"\"\"\n",
    "\n",
    "    def decorates(main_class, exclude, appended_class):\n",
    "        if main_class is None:\n",
    "            main_class = globals()[appended_class.__name__]\n",
    "        for k, v in appended_class.__dict__.items():\n",
    "            if k not in exclude:\n",
    "                setattr(main_class, k, v)\n",
    "        return main_class\n",
    "\n",
    "    return functools.partial(decorates, main_class, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleClass:\n",
    "    def test1(self):\n",
    "        url = sess.get(\"http://www.heise.de\")\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to tor..\n",
      "Current IP:{'origin': '89.163.143.8'}\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "@update_class()\n",
    "class ExampleClass:\n",
    "    def test2(self):\n",
    "        with TorRequests() as tor_requests:\n",
    "            print(\"Connecting to tor..\")\n",
    "            global sess\n",
    "            with tor_requests.get_session() as sess: #sess pulls a request of the page with the tor connection\n",
    "                current_ip = sess.get(\"http://httpbin.org/ip\").json()\n",
    "                print(f\"Current IP:{current_ip}\")\n",
    "                self.test1()\n",
    "\n",
    "ExampleClass().test2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caae7e249fc351f228f30c0ea597686a1b1aad5e394e945128343f5494793c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
