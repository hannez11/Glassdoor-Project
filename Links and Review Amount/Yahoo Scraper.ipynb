{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab glassdoor links and review amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from torpy.http.requests import TorRequests\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_raw = pd.read_csv(r\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\0725_UniqueFirmList_raw.csv\", sep=\";\", decimal=\",\", encoding='unicode_escape')\n",
    "\n",
    "## sort\n",
    "input_raw.sort_values(by=[\"2008_2022\", \"EnglishSpeaking\", \"Max_Cosine_Similarity\"], ascending=[0,0,0], inplace=True)\n",
    "input_raw.reset_index(inplace=True)\n",
    "\n",
    "## filter\n",
    "input = input_raw[[\"CompanyName\", \"ISIN\", \"Link_Marius\", \"Max_Cosine_Similarity\", \"2008_2022\", \"EnglishSpeaking\"]]\n",
    "\n",
    "## move ticker (written in brackets for some firms) to new column\n",
    "if 'Ticker' not in input.columns:\n",
    "    input.insert(input.columns.get_loc(\"CompanyName\")+1, \"Ticker\", np.nan)\n",
    "    input[['CompanyName', 'Ticker']] = input['CompanyName'].str.split('(', 1, expand=True)\n",
    "    input[\"CompanyName\"] = input[\"CompanyName\"].str.replace(\"&\", \"and\")\n",
    "    input[\"Ticker\"] = input[\"Ticker\"].str.replace(\")\", \"\")\n",
    "\n",
    "## replace sonderzeichen\n",
    "sonderzeichen = {\"àö¬ß\": \"ä\", \"‚àö‚â•\": \"o\"}\n",
    "input.replace({\"CompanyName\": sonderzeichen})\n",
    "\n",
    "## remove legal addons in firm names\n",
    "input[\"CompanyName\"] = input[\"CompanyName\"].str.replace(\",| Inc[.]| Corp[.]| Limited| Ltd[.]| S[.]A[.]| AG| Co[.]| plc| LP| LLC| L[.]L[.]C[.]| Corporation| Holdings\", \"\", regex=True, case=False) #. is used a a placeholder. -> put it in character class by using []\n",
    "########### OHNE CLEANING NOCHMAL DURCHLAUFEN LASSEN, BSPW BEI Foreign Currency Exchange Corp. ist das Corp wichtig!\n",
    "\n",
    "# input\n",
    "\n",
    "## save file with dynamic file name\n",
    "path_with_time = Path.joinpath(Path.cwd().parent, f'{time.strftime(\"%m%d\")}_UniqueFirmList.csv')\n",
    "#\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\0808_UniqueFirmList_HannesLinks_1500-1509.csv\"\n",
    "input.to_csv(path_with_time, sep=\";\", decimal=\",\", index=False)\n",
    "print(f\"{path_with_time} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv(r\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\0813_UniqueFirmList.csv\", sep=\";\", decimal=\",\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yahoo scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proxy rotation via tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yahoo_scraper(df, sleeptime_max=1, tor_active=\"no\"):\n",
    "    try: #if operation fails, then return the current df\n",
    "        firmname = df[\"CompanyName\"]\n",
    "        print(f\"Firm #{df.name + 1}: {firmname}\")\n",
    "        # query = f'site:glassdoor.com {firmname}'.replace(' ', '+')\n",
    "        query = f'site:glassdoor.com \"{firmname}\"'.replace(' ', '+') #putting {firmname} around \"\" might help. however, the query might also be too strict not returning any results\n",
    "        URL = f\"https://search.yahoo.com/search?p={query}&vc=en&pz=20\"\n",
    "\n",
    "        ## shuffle through user agent list for bot prevention. use IP shuffling to prevent further bot detection\n",
    "        time.sleep(round(random.uniform(0.25,sleeptime_max), 3))\n",
    "        USER_AGENTS = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0\",\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:103.0) Gecko/20100101 Firefox/103.0\",\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36\",\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 Edg/103.0.1264.71\"\n",
    "                    ]\n",
    "        headers = {\"user-agent\":random.choice(USER_AGENTS)}\n",
    "\n",
    "        ## use tor\n",
    "        if tor_active==\"yes\":\n",
    "            url = sess.get(URL, headers=headers)\n",
    "        ## use regular IP\n",
    "        else:\n",
    "            url = requests.get(URL, headers=headers)\n",
    "\n",
    "        soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "        time.sleep(0.2) #to fully load the page\n",
    "\n",
    "        ## scraper ##\n",
    "        #############\n",
    "\n",
    "        link_found = 0\n",
    "        ## check if amount of ratings is displayed in at least of the search results. if so, grab this number and the parent div containing the link\n",
    "        if soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) and len(soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}).text)<15: #check length of the classes text, since class is not exclusively used for amount of ratings\n",
    "            ## grab the review number\n",
    "            review_amount = soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) #only get the first item, since it appears to be most accurate. could be of the last search result\n",
    "            try: #sometimes \"Currency: EUR\" is displayed instead of the review amount\n",
    "                review_amount_final = review_amount.text.split(\"(\")[1][:-1] #\"4/5 (3)\"\n",
    "                if \"K\" in review_amount_final: # \"5.7K\" to 5700\n",
    "                    review_amount_final = review_amount_final.replace(\".\", \"\")\n",
    "                    review_amount_final = review_amount_final.replace(\"K\", \"00\")\n",
    "                # print(f\"review amount: {review_amount_final}\")\n",
    "                df[\"review_amount\"] = review_amount_final\n",
    "                ## grab the href from the parent of the same search result\n",
    "            except:\n",
    "                df[\"review_amount\"] = \"\" \n",
    "            review_parents = review_amount.find_parents(\"div\")\n",
    "            for link in review_parents:\n",
    "                if link.find(\"a\") and \"glassdoor\" in link.find(\"a\")[\"href\"]:\n",
    "                    alink = link.find(\"a\")\n",
    "                    link_found = 1\n",
    "                    break\n",
    "        ## if amount of ratings is not on page, then just grab the href\n",
    "        elif link_found != 1:\n",
    "            # print(\"review amount: none found\")\n",
    "            results = soup.findAll(\"h3\")\n",
    "            for link in results:\n",
    "                if link.find(\"a\") and \"glassdoor\" in link.find(\"a\")[\"href\"]:\n",
    "                    alink = link.find(\"a\")\n",
    "                    link_found = 1\n",
    "                    break\n",
    "        ## if neither link nor review amount was found\n",
    "            if link_found != 1:\n",
    "                link_found = \"no link found\" #\"no link found\"\n",
    "                link_final = \"\"\n",
    "                print(\"no link found\")\n",
    "\n",
    "        \n",
    "        ## decode href link if link was found ##\n",
    "\n",
    "        if link_found == 1:\n",
    "            link_with_yahoo = unquote(alink[\"href\"]) #unquote decodes URLs\n",
    "            try: #weird cases in which an image link is retrieved\n",
    "                link_final = link_with_yahoo.split(\"RU=\")[1].split(\"/RK=2\")[0] #https://r.search.yahoo.com/_ylt=A0geK9dD9t9i2BIAtodXNyoA;_ylu=Y29sbwNiZjEEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1658873541/RO=10/RU=https://www.glassdoor.com/Overview/Working-at-California-Micro-Devices-EI_IE1221.11,35.htm/RK=2/RS=Ss7CVR85Jhnt899NkWVSw9eSVUw-\n",
    "                if \"/Working\" in link_final:\n",
    "                    # from https://www.glassdoor.com/Overview/Working-at-Tesla-EI_IE43129.11,16.htm\n",
    "                    # to https://www.glassdoor.com/Reviews/Tesla-Reviews-E43129.htm\n",
    "                    link_final = link_final.replace(\"/Overview/Working-at-\", \"/Reviews/\") #shouldnt be an issue to replace the /Overview/ part with /Reviews/ even though the original Reviews link looks different\n",
    "                    link_final = link_final.replace(\"EI_IE\", \"Reviews-E\") \n",
    "                    link_final = link_final.split(\".htm\")[0][:-6] + \".htm\"\n",
    "                    link_found = \"transformed overview link\"\n",
    "                elif \"/Jobs/\" in link_final:\n",
    "                    link_final = link_final.replace(\"/Jobs/\", \"/Reviews/\")\n",
    "                    link_final = link_final.replace(\"-Jobs-\", \"-Reviews-\")\n",
    "                    link_found = \"transformed jobs link\"\n",
    "                elif \"/Salary/\" in link_final:\n",
    "                    link_final = link_final.replace(\"/Salary/\", \"/Reviews/\")\n",
    "                    link_final = link_final.replace(\"-Salaries-\", \"-Reviews-\")\n",
    "                    link_found = \"transformed salary link\"\n",
    "                elif \"/Reviews/\" in link_final:\n",
    "                    link_found = \"original review link\"\n",
    "                else:\n",
    "                #elif \"/job-listing/\" or \"/Job/\" in link_final:\n",
    "                    link_found = \"wrong link\"\n",
    "            except:\n",
    "                link_final = link_with_yahoo\n",
    "                \n",
    "        df[\"Link_Hannes\"] = link_final\n",
    "        df[\"link_found\"] = link_found\n",
    "        print(link_final, \"\\n\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        return df\n",
    "\n",
    "# yahoo_scraper(\"El Paso Pipeline Partners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set parameters\n",
    "startrow, endrow = 7000, 8000\n",
    "tor = \"yes\" # change to yes or no\n",
    "sleeptime_max = 0.5 #use >2 with original ip (tor = \"no\"). around 1000 requests possible until temp. ip block\n",
    "\n",
    "\n",
    "## create a copy of the input frame; keep old index as extra column\n",
    "input_copy = input[startrow:endrow].reset_index(drop=False)\n",
    "\n",
    "input_copy[\"Link_Hannes\"] = np.nan\n",
    "input_copy[\"review_amount\"] = np.nan\n",
    "input_copy[\"link_found\"] = np.nan\n",
    "\n",
    "## execute yahoo scraper for each row by using apply ##\n",
    "#######################################################\n",
    "\n",
    "## with Tor-Gateway\n",
    "if tor == \"yes\":\n",
    "    with TorRequests() as tor_requests:\n",
    "        print(\"Connecting to tor..\")\n",
    "        with tor_requests.get_session() as sess: #sess pulls a request of the page with the tor connection\n",
    "            current_ip = sess.get(\"http://httpbin.org/ip\").json()\n",
    "            print(f\"Current IP:{current_ip}; sleeptime: {sleeptime_max}\\n\")\n",
    "            input_copy = input_copy.apply(yahoo_scraper, args=(sleeptime_max, \"yes\"), axis=1) #sess method can be executed from this sibling function\n",
    "## without tor\n",
    "else:\n",
    "    input_copy = input_copy.apply(yahoo_scraper, args=(sleeptime_max, \"no\"), axis=1)\n",
    "print(f\"Scraping finished for {startrow} to {endrow}\")\n",
    "    \n",
    "\n",
    "## reorder columns\n",
    "# input_copy.keys() #['index', 'CompanyName', 'Ticker', 'ISIN', 'Link_Marius', 'Max_Cosine_Similarity', '2008_2022', 'EnglishSpeaking', 'Link_Hannes', 'review_amount', 'link_found']\n",
    "input_copy = input_copy[['index', 'ISIN', 'Ticker', '2008_2022', 'EnglishSpeaking', 'CompanyName', 'Max_Cosine_Similarity', 'Link_Marius', 'Link_Hannes', 'link_found', 'review_amount']]\n",
    "\n",
    "\n",
    "## save file with dynamic file name\n",
    "path_with_time = Path.joinpath(Path.cwd().parent, f'{time.strftime(\"%m%d\")}_UniqueFirmList_HannesLinks_{startrow}-{endrow}_strict.csv')\n",
    "#\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\0808_UniqueFirmList_HannesLinks_1500-1509.csv\"\n",
    "input_copy.to_csv(path_with_time, sep=\";\", decimal=\",\", index=False)\n",
    "print(f\"{path_with_time} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_with_time = Path.joinpath(Path.cwd().parent, f'{time.strftime(\"%m%d\")}_UniqueFirmList_HannesLinks_{startrow}-{endrow}.csv')\n",
    "#\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\0808_UniqueFirmList_HannesLinks_1500-1509.csv\"\n",
    "input_copy.to_csv(path_with_time, sep=\";\", decimal=\",\", index=False)\n",
    "print(f\"{path_with_time} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append multiple csv files to singular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "# create an empty pandas data frame\n",
    "df_complete = pd.DataFrame()\n",
    "\n",
    "dir = r\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\HannesLinksStrict\"\n",
    " \n",
    "# iterate over all files within folder\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_complete = pd.concat([df_complete , pd.read_csv(os.path.join(dir, file), sep=\";\", decimal=\",\", encoding='unicode_escape' )], axis=0 )\n",
    "df_complete.reset_index(drop=True, inplace=True) # reset the index \n",
    " \n",
    "## save\n",
    "path_with_time = Path.joinpath(Path.cwd().parent, f'{time.strftime(\"%m%d\")}_UniqueFirmList_HannesLinks_English-Post2008_Strict.csv')\n",
    "#\"W:\\019_Glassdoor\\1 Data\\1 Glassdoor Links\\0808_UniqueFirmList_HannesLinks_1500-1509.csv\"\n",
    "df_complete.to_csv(path_with_time, sep=\";\", decimal=\",\", index=False)\n",
    "print(f\"{path_with_time} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example: apply a function to a column with 2 output columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = input.copy()\n",
    "\n",
    "## output only one column\n",
    "# def yahoo_scraper(x):\n",
    "#     return x+2\n",
    "\n",
    "# df_example[\"new\"] = df_example[\"Max_Cosine_Similarity\"].apply(yahoo_scraper)\n",
    "# df_example\n",
    "\n",
    "\n",
    "def example_function(df):\n",
    "    df[\"example_result1\"] = df[\"Max_Cosine_Similarity\"] + 1\n",
    "    df[\"example_result2\"] = 2\n",
    "    return df\n",
    "\n",
    "df_example = df_example.apply(example_function, axis=1)\n",
    "df_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proxy rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tor-Gateway\n",
    "from torpy.http.requests import TorRequests\n",
    "\n",
    "with TorRequests() as tor_requests:\n",
    "    print(\"Connecting..\")\n",
    "    with tor_requests.get_session() as sess:\n",
    "        current_ip = sess.get(\"http://httpbin.org/ip\").json()\n",
    "        print(f\"Current IP:{current_ip}\")\n",
    "\n",
    "        URL = \"https://search.yahoo.com/search?p=site:glassdoor.com 'Tesla'&vc=en&pz=20\"\n",
    "        url = sess.get(URL, timeout=5)\n",
    "        soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "        \n",
    "        review_amount = soup.find(\"li\", {\"class\":[\"tc\", \"bxz-bb\"]}) #only get the first item, since it appears to be most accurate. could be of the last search result\n",
    "        print(review_amount)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caae7e249fc351f228f30c0ea597686a1b1aad5e394e945128343f5494793c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
